{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "486e558c-e164-4026-9128-a409d67b9eca",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Exploratory Data Analysis (EDA) is conducted to gain insights and understand the underlying patterns, relationships, and characteristics of a dataset. It helps in forming initial hypotheses, identifying data quality issues, and guiding further analysis. Here are some common questions that EDA aims to answer:\n",
    "\n",
    "- **What are the main features and variables in the dataset?** EDA helps in understanding the structure of the data and identifying the different variables and their types.\n",
    "\n",
    "- **What is the distribution of the variables?** EDA allows for the examination of the data distribution, such as checking for normality, skewness, or multimodality, which can provide insights into the nature of the variables.\n",
    "\n",
    "- **Are there any missing values or data quality issues?** EDA helps in detecting missing values, outliers, inconsistencies, or other data quality problems that may require cleaning or imputation.\n",
    "\n",
    "- **What is the relationship between variables?** EDA explores the associations and correlations between variables, which can uncover patterns, dependencies, or potential predictive relationships.\n",
    "\n",
    "- **Are there any trends or patterns over time?** EDA investigates time-dependent data to identify trends, seasonality, cyclic patterns, or other temporal characteristics.\n",
    "\n",
    "- **Are there any differences or patterns across different groups or categories?** EDA allows for the comparison and analysis of data based on different groups or categories, such as demographic variables or experimental conditions.\n",
    "\n",
    "- **Are there any outliers or extreme observations?** EDA helps in identifying outliers or extreme values that may require further investigation to determine if they are valid data points or errors.\n",
    "\n",
    "- **What are the summary statistics or descriptive measures of the variables?** EDA calculates summary statistics, such as mean, median, standard deviation, and percentiles, to describe the central tendency, variability, and shape of the data.\n",
    "\n",
    "- **Are there any patterns or clusters in the data?** EDA explores the data for potential clusters or groups based on similarity or proximity to uncover hidden patterns or subpopulations.\n",
    "\n",
    "- **What are the limitations or potential biases in the data?** EDA provides an opportunity to assess the limitations, biases, or sampling issues in the data, which helps in interpreting the results accurately.\n",
    "\n",
    "These questions are not exhaustive, and the specific questions to be answered through EDA depend on the dataset, domain, and research or analysis objectives. EDA is a flexible and iterative process that allows for exploratory investigation and hypothesis generation before further analysis or modeling.\n",
    "\n",
    "\n",
    "This notebook provides a starting template for exploratory data analysis.\n",
    "Implementation details for EDA will vary based on the usecase but this notebook can be used as a starting point for any use case.\n",
    "To use the notebook for a new use case, fill in the relevant blanks / parameters in the notebook or add relevant additional steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89c882-1080-4b1d-baa2-4e13cb472d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6555623-2169-41a0-9f55-382108b0ef7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Data\n",
    "\n",
    "Data could be imported from Google Cloud Storage (GCS), local csv file or Bigquery. Managing data via Bigquery is the recommended appraoch for tabular data as it is easier to maintain and update the data as compared to csv files stores locally or in GCs. For other data types like images or text, GCS will be the input source.\n",
    "\n",
    "For importing data from Bigquery, we can import the necessary modules and functions directly into the notebook from src. These functions can be quickly and easily accessed and run with defined input parameters. Their output can be examined and used as inputs for other functions. With this the ML training, evaluation or inference flow can be easily run using the already refactored code.\n",
    "\n",
    "Below is the sample for importing data from Bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525fca2-6768-4674-8e5d-7f4eab6ee6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "from xgb_churn_prediction.data import data_ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae54acd-11a9-4293-b3a3-538369959e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "\n",
    "# our GCP project ID - currently 'ncau-data-nprod-aitrain'\n",
    "PROJECT_ID=\"nca-datapl-nprod-dev-genai\"\n",
    "# the region of our gcp project\n",
    "LOCATION=\"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c0cc7b-0071-42eb-abee-5b54bd8f11b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the BigQuery query\n",
    "data_query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `project_id.your_dataset.your_table`\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Optional Dictionary to set the dataype mapping when reading the data\n",
    "Dictionary format: <column_name>:<datatype>\"\"\"\n",
    "\n",
    "dtypes = {'column_name1': int, 'column_name2': float}\n",
    "\n",
    "data_df = data_ingestion.execute_bq_query(PROJECT_ID, data_query, dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "920d554a-65d4-4826-9c98-c1204f8167f8",
   "metadata": {},
   "source": [
    "### Dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2929e1-cb9b-42aa-8f15-41ca1a722fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of rows, columns\n",
    "print(data_df.shape)\n",
    "\n",
    "# Print datatype for columns and index information\n",
    "print(data_df.info())\n",
    "\n",
    "# Look at first 10 rows of dataframe\n",
    "data_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dad6c43a-45f4-4d9b-939a-777a7e03f858",
   "metadata": {},
   "source": [
    "**Observation** : Log any relevant comments / observations if applicable\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e637b1b7-3509-449a-90aa-294a55f0fe36",
   "metadata": {},
   "source": [
    "### Descriptive statistics for all numeric columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e59de4-b40a-4010-afdb-d10b511f5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18ead15d-ec99-4ca0-b96a-306c0d92dc72",
   "metadata": {},
   "source": [
    "**Observation** : Log any relevant comments / observations if applicable\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8d2c56e-dcf1-4f26-b45e-efba5312e4b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d01edd0-536d-4e70-ac81-a389692323dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Checking for null values\n",
    "\n",
    "Caculating percentage of rows with null values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3198fcf-d7e3-43d1-9169-ef12740c48b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_df.isna().mean() * 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e42ba12-56b1-487e-b308-ee182730ce57",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Handle null values\n",
    "\n",
    "There are several ways to handle missing data in Python. Here are some common techniques:\n",
    "\n",
    "1. Dropping missing values:\n",
    "\n",
    "    - Use dropna() function from pandas to remove rows or columns with missing values. For example, data.dropna() will drop all rows containing any missing values.\n",
    "    - You can specify the axis and threshold for dropping. For example, data.dropna(axis=1, thresh=1000) will drop columns that have less than 1000 non-null values.\n",
    "   \n",
    "2. Filling missing values:\n",
    "\n",
    "    - Use fillna() function from pandas to fill missing values with a specific value or strategy. For example, data.fillna(0) will fill all missing values with 0.\n",
    "    - You can also use statistical measures like mean, median, or mode to fill missing values. For example, data['column1'].fillna(data['column1'].mean()) will fill missing values in a specific column with the mean value of that column.\n",
    "\n",
    "3. Interpolation\n",
    "\n",
    "    - Use interpolation methods to estimate missing values based on the existing data. Pandas provides various interpolation methods such as linear, quadratic, and cubic. For example, data['column1'].interpolate(method='linear') will fill missing values using linear interpolation in a specific column.\n",
    "\n",
    "4. Imputation:\n",
    "\n",
    "    - Use advanced imputation techniques to estimate missing values based on the relationships between variables. The scikit-learn library provides the SimpleImputer class for imputation.\n",
    "    \n",
    "\n",
    "It's important to choose the appropriate method based on the characteristics of your dataset and the nature of the missing data. Consider the amount of missing data, the impact of missing values on your analysis, and the assumptions you can make about the data when deciding how to handle missing data.\n",
    "\n",
    "Below are some methods that can be used for handling missing values based on the usecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf39327-fad3-4b09-83eb-bac4c2739583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_values(dataframe, axis=0, thresh=None):\n",
    "    # Drop rows or columns with missing values\n",
    "    return dataframe.dropna(axis=axis, thresh=thresh)\n",
    "\n",
    "def fill_missing_values(dataframe, value=None, method=None):\n",
    "    # Fill missing values with a specific value or strategy\n",
    "    if value is not None:\n",
    "        return dataframe.fillna(value)\n",
    "    elif method is not None:\n",
    "        if method == 'mean':\n",
    "            return dataframe.fillna(dataframe.mean())\n",
    "        elif method == 'median':\n",
    "            return dataframe.fillna(dataframe.median())\n",
    "        elif method == 'mode':\n",
    "            return dataframe.fillna(dataframe.mode().iloc[0])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose 'mean', 'median', or 'mode'.\")\n",
    "    else:\n",
    "        raise ValueError(\"Specify either a value or a method.\")\n",
    "\n",
    "def interpolate_missing_values(dataframe, method='linear'):\n",
    "    # Interpolate missing values using interpolation methods\n",
    "    return dataframe.interpolate(method=method)\n",
    "\n",
    "def impute_missing_values(dataframe, strategy='mean'):\n",
    "    # Impute missing values using advanced imputation techniques\n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    return pd.DataFrame(imputer.fit_transform(dataframe), columns=dataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95b13fc-38ca-4c3b-a89e-05d7eae881d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = drop_missing_values(data_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7ad1ccf-294e-490a-bddc-dff55857a80b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check for missing data for specific dates (Only applicable for time series data)\n",
    "\n",
    "If your time series data is expected to have a regular frequency (e.g., daily, monthly), checking for missing dates is important. Missing dates can introduce gaps in the time series, affecting the accuracy of any time-based calculations or analyses. In such cases, it's crucial to identify and handle missing dates appropriately.\n",
    "\n",
    "If your time series data comes from a data collection process where missing dates are expected or occur naturally, it might be acceptable to have missing dates. For example, if you have daily sales data, missing dates on weekends or public holidays could be considered normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708a3a0-a693-4612-b554-20774dbae938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_dates(dataframe, date_column, group_column=None):\n",
    "    \"\"\"\n",
    "    Check for missing dates in a time series DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): The DataFrame containing the time series data.\n",
    "        date_column (str): The name of the column representing the dates.\n",
    "        group_column (str, optional): The name of the column to group the data by (default: None).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Prints the missing dates in the time series data. If `group_column` is None, missing dates\n",
    "    are checked for the entire DataFrame. If `group_column` is specified, missing dates are\n",
    "    checked for each group separately.\n",
    "    \n",
    "    Note:\n",
    "        - The function converts the date column to datetime type if necessary.\n",
    "        - The function uses pandas date_range to create a complete date range based on the minimum\n",
    "          and maximum dates in the DataFrame or group.\n",
    "        - Missing dates are identified by comparing the complete date range with the DataFrame's\n",
    "          or group's date column using the isin method.\n",
    "\n",
    "    Example:\n",
    "        data = {\n",
    "            'date': ['2023-01-01', '2023-01-02', '2023-01-04', '2023-01-01', '2023-01-03', '2023-01-04'],\n",
    "            'value': [10, 20, 30, 40, 50, 60],\n",
    "            'group': ['A', 'A', 'A', 'B', 'B', 'B']\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        check_missing_dates(df, 'date')\n",
    "        print()\n",
    "        check_missing_dates(df, 'date', 'group')\n",
    "    \"\"\"\n",
    "    # Convert the date column to datetime type if necessary\n",
    "    if not pd.core.dtypes.common.is_datetime_or_timedelta_dtype(dataframe[date_column]):\n",
    "        dataframe[date_column] = pd.to_datetime(dataframe[date_column])\n",
    "        \n",
    "    if group_column is None:\n",
    "        # If no group column is specified, check missing dates for the entire DataFrame\n",
    "        complete_date_range = pd.date_range(start=dataframe[date_column].min(), end=dataframe[date_column].max(), freq='D')\n",
    "\n",
    "        missing_dates = complete_date_range[~complete_date_range.isin(dataframe[date_column])]\n",
    "\n",
    "        if missing_dates.empty:\n",
    "            print(\"No missing dates found.\")\n",
    "        else:\n",
    "            print(\"Missing dates:\")\n",
    "            print(missing_dates)\n",
    "\n",
    "    else:\n",
    "        # If a group column is specified, check missing dates for each group\n",
    "        groups = dataframe.groupby(group_column)\n",
    "\n",
    "        for group_name, group_data in groups:\n",
    "            print(group_data)\n",
    "            complete_date_range = pd.date_range(start=group_data[date_column].min(), end=group_data[date_column].max(), freq='D')\n",
    "\n",
    "            missing_dates = complete_date_range[~complete_date_range.isin(group_data[date_column])]\n",
    "\n",
    "            if missing_dates.empty:\n",
    "                print(f\"No missing dates found for group: {group_name}\")\n",
    "            else:\n",
    "                print(f\"Missing dates for group: {group_name}\")\n",
    "                print(missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9526573-2145-429c-a12b-17cb8db76428",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_dates(data_df, 'date_column_name')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e12cc28-b7e7-41b7-a8eb-7423e4a64f9e",
   "metadata": {},
   "source": [
    "**Observation** : Log any relevant comments / observations if applicable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d3784bf-f6cc-453e-bca0-0c03e1a2c40e",
   "metadata": {},
   "source": [
    "### Check for outliers\n",
    "\n",
    "There are several approaches to check for outliers in a dataset. Here are some commonly used methods:\n",
    "\n",
    "1. **Z-Score Method**: Calculate the z-score for each data point by subtracting the mean and dividing by the standard deviation. Data points with z-scores above a certain threshold (e.g., 2 or 3) are considered outliers.\n",
    "\n",
    "2. **Modified Z-Score Method**: Similar to the z-score method, but instead of using the mean and standard deviation, it uses the median and median absolute deviation (MAD) to calculate the modified z-score. Data points with modified z-scores above a threshold (e.g., 2.5 or 3) are considered outliers.\n",
    "\n",
    "3. **Percentiles/Quartiles**: Compute the lower and upper quartiles and the interquartile range (IQR). Data points below the lower quartile minus a certain multiplier times the IQR or above the upper quartile plus a certain multiplier times the IQR are considered outliers. Common multipliers are 1.5 or 3.\n",
    "\n",
    "4. **Box Plots**: Create box plots to visualize the distribution of the data. Outliers are typically identified as points that fall outside the whiskers of the box plot.\n",
    "\n",
    "5. **Histograms**: Generate histograms or density plots to visualize the frequency distribution of the data. Unusually high or low values that appear far away from the bulk of the data can be potential outliers.\n",
    "\n",
    "6. **Domain Knowledge and Business Rules**: Apply domain-specific knowledge or business rules to identify outliers. For example, in a sales dataset, unusually high or low sales figures may be considered outliers based on predetermined rules.\n",
    "\n",
    "7. **Machine Learning Models**: Train a machine learning model (e.g., linear regression, random forest) and examine the residuals or errors. Unusually large residuals or errors can indicate the presence of outliers.\n",
    "\n",
    "8. **Cluster Analysis**: Perform cluster analysis to group similar data points together. Outliers can be identified as points that do not belong to any cluster or form their own separate cluster.\n",
    "\n",
    "It is important to note that no single method is foolproof, and the choice of approach depends on the nature of the data, the specific problem domain, and expert knowledge. It is often recommended to use multiple methods and consider the context of the data before determining which data points are truly outliers.\n",
    "\n",
    "Below is an implementation plotting the box plot and histogram and calculating the outliers based on modified Z Score as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd6bf6-3d63-4e67-957a-4f90397f6fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outliers(dataframe, column, group_column=None):\n",
    "    \"\"\"\n",
    "    Check for outliers in a DataFrame column, optionally aggregated by a group column.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): The DataFrame containing the data.\n",
    "        column (str): The name of the column to check for outliers.\n",
    "        group_column (str, optional): The name of the column to group the data by (default: None).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Calculates summary statistics, creates box plots and histograms, and identifies outliers\n",
    "    using z-score and modified z-score methods. Outliers are printed for each group if\n",
    "    `group_column` is specified.\n",
    "\n",
    "    Example:\n",
    "        data = {\n",
    "            'group': ['A', 'A', 'B', 'B', 'B', 'C', 'C'],\n",
    "            'value': [10, 20, 5, 30, 40, 100, 110]\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        check_outliers(df, 'value')\n",
    "        print()\n",
    "        check_outliers(df, 'value', 'group')\n",
    "    \"\"\"\n",
    "    if group_column is None:\n",
    "        # Calculate summary statistics\n",
    "        summary_stats = dataframe[column].describe()\n",
    "        print(\"Summary Statistics:\")\n",
    "        print(summary_stats)\n",
    "        print()\n",
    "\n",
    "        # Create box plot\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.boxplot(data=dataframe, y=column)\n",
    "        plt.title(\"Box Plot\")\n",
    "        plt.show()\n",
    "\n",
    "        # Create histogram\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.histplot(data=dataframe, x=column, kde=True)\n",
    "        plt.title(\"Histogram\")\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate z-scores\n",
    "        z_scores = (dataframe[column] - np.mean(dataframe[column])) / np.std(dataframe[column])\n",
    "\n",
    "        # Define threshold for z-score method\n",
    "        z_score_threshold = 3\n",
    "\n",
    "        # Identify outliers using z-score method\n",
    "        outliers_zscore = dataframe[np.abs(z_scores) > z_score_threshold]\n",
    "        print(\"Outliers (Z-score method):\")\n",
    "        print(outliers_zscore)\n",
    "    else:\n",
    "        # Group by the specified column\n",
    "        grouped_data = dataframe.groupby(group_column)\n",
    "\n",
    "        for group_name, group_data in grouped_data:\n",
    "            print(f\"Group: {group_name}\")\n",
    "            print(\"-------------\")\n",
    "\n",
    "            # Calculate summary statistics for each group\n",
    "            summary_stats = group_data[column].describe()\n",
    "            print(\"Summary Statistics:\")\n",
    "            print(summary_stats)\n",
    "            print()\n",
    "\n",
    "            # Create box plot for each group\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            sns.boxplot(data=group_data, x=group_column, y=column)\n",
    "            plt.title(f\"Box Plot - Group {group_name}\")\n",
    "            plt.show()\n",
    "\n",
    "            # Create histogram for each group\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            sns.histplot(data=group_data, x=column, kde=True)\n",
    "            plt.title(f\"Histogram - Group {group_name}\")\n",
    "            plt.show()\n",
    "\n",
    "            # Calculate z-scores for each group\n",
    "            z_scores = (group_data[column] - np.mean(group_data[column])) / np.std(group_data[column])\n",
    "\n",
    "            # Define threshold for z-score method\n",
    "            z_score_threshold = 3\n",
    "\n",
    "            # Identify outliers using z-score method for each group\n",
    "            outliers_zscore = group_data[np.abs(z_scores) > z_score_threshold]\n",
    "            print(\"Outliers (Z-score method):\")\n",
    "            print(outliers_zscore)\n",
    "\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bf1d4b-e703-43f3-91bf-3cf647c066ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_outliers(data_df, 'value_column_name')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ede97ca7-c560-4f64-a89f-314a05e9cb8f",
   "metadata": {},
   "source": [
    "**Observation** : Log any relevant comments / observations if applicable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f54ced1-f4b8-4ffb-9dd6-350c5b2e826f",
   "metadata": {},
   "source": [
    "### Handling outliers\n",
    "\n",
    "\n",
    "Handling outliers depends on the specific context and goals of your analysis. Here are some common approaches for dealing with outliers:\n",
    "\n",
    "- Leave the Outliers: In some cases, outliers may be valid and meaningful data points. If you have confidence in the accuracy and relevance of the outliers, you can choose to leave them as they are and proceed with your analysis. However, it's important to interpret the results with caution, as outliers can have a significant impact on statistical measures and models.\n",
    "\n",
    "- Remove the Outliers: If the outliers are due to data entry errors, measurement errors, or other anomalies that are unlikely to represent true values, you can remove them from your dataset. However, it's crucial to document and justify the reason for their removal to ensure transparency and reproducibility of your analysis.\n",
    "\n",
    "- Transform the Data: Instead of removing outliers, you can apply data transformations to make the data distribution more suitable for analysis. For example, you can use logarithmic or square root transformations to reduce the impact of extreme values while preserving the overall patterns in the data.\n",
    "\n",
    "- Imputation: If you decide to keep the outliers in your dataset, you can consider imputing their values instead of removing them. Imputation involves estimating missing or extreme values based on other data points or statistical techniques. Various imputation methods, such as mean imputation, median imputation, or regression imputation, can be used depending on the characteristics of your data.\n",
    "\n",
    "- Separate Analysis: In some cases, outliers may represent a distinct subgroup within your data that requires separate analysis. You can consider analyzing the outliers separately or applying different techniques tailored to their specific characteristics.\n",
    "\n",
    "It's important to remember that the appropriate approach for handling outliers depends on the specific dataset, the analysis objectives, and the domain knowledge. It's recommended to consider the impact of outlier handling on your analysis results and validate the robustness of your findings with and without outlier treatment.\n",
    "\n",
    "Below are some methods that showcase how to apply some of these techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce4cc6-c49c-429c-86bc-b9f76b0a3371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers_remove(dataframe, column, method='iqr', multiplier=1.5):\n",
    "    # Remove outliers using specified method and multiplier\n",
    "    if method == 'iqr':\n",
    "        q1 = dataframe[column].quantile(0.25)\n",
    "        q3 = dataframe[column].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - multiplier * iqr\n",
    "        upper_bound = q3 + multiplier * iqr\n",
    "        return dataframe[(dataframe[column] >= lower_bound) & (dataframe[column] <= upper_bound)]\n",
    "    elif method == 'std':\n",
    "        z_scores = (dataframe[column] - np.mean(dataframe[column])) / np.std(dataframe[column])\n",
    "        return dataframe[np.abs(z_scores) <= multiplier]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'iqr' or 'std'.\")\n",
    "\n",
    "def handle_outliers_transform(dataframe, column, method='log'):\n",
    "    # Apply data transformation to handle outliers\n",
    "    if method == 'log':\n",
    "        dataframe[column] = np.log1p(dataframe[column])\n",
    "    elif method == 'sqrt':\n",
    "        dataframe[column] = np.sqrt(dataframe[column])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'log' or 'sqrt'.\")\n",
    "    return dataframe\n",
    "\n",
    "def handle_outliers_impute(dataframe, column, method='mean'):\n",
    "    # Impute the outliers with specified method\n",
    "    if method == 'mean':\n",
    "        dataframe[column] = np.where(\n",
    "            np.abs(dataframe[column] - np.mean(dataframe[column])) > 2 * np.std(dataframe[column]),\n",
    "            np.mean(dataframe[column]),\n",
    "            dataframe[column]\n",
    "        )\n",
    "    elif method == 'median':\n",
    "        dataframe[column] = np.where(\n",
    "            np.abs(dataframe[column] - np.median(dataframe[column])) > 2 * np.median(dataframe[column]),\n",
    "            np.median(dataframe[column]),\n",
    "            dataframe[column]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'mean' or 'median'.\")\n",
    "    return dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef0deac2-5a08-4799-b98d-f65e66ee4650",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Analysis and Visualisation\n",
    "\n",
    "As part of Exploratory Data Analysis (EDA), various data visualizations can provide valuable insights into the dataset. Here are some common types of visualizations that can be helpful during EDA:\n",
    "\n",
    "- **Histograms**: Histograms display the distribution of a numerical variable by dividing it into bins and showing the frequency or count of values within each bin. They help to understand the shape, central tendency, and spread of the data.\n",
    "\n",
    "- **Box plots**: Box plots provide a summary of the distribution of a numerical variable by displaying the median, quartiles, and any outliers. They help identify the presence of outliers, skewness, and overall variability in the data.\n",
    "\n",
    "- **Scatter plots**: Scatter plots show the relationship between two numerical variables. They help visualize patterns, correlations, and potential outliers. Scatter plots can also be enhanced with color or size encoding to represent additional dimensions.\n",
    "\n",
    "- **Line plots**: Line plots are useful for visualizing trends and changes over time or other ordered categories. They can show how a variable evolves and reveal patterns, seasonality, or anomalies.\n",
    "\n",
    "- **Bar charts**: Bar charts are suitable for visualizing categorical variables by displaying the count or proportion of each category. They help compare different categories and identify the most frequent or significant ones.\n",
    "\n",
    "- **Heatmaps**: Heatmaps provide a visual representation of the relationship between two categorical variables. They use color to represent the frequency or proportion of each combination of categories, allowing the identification of associations or dependencies.\n",
    "\n",
    "- **Correlation matrices**: Correlation matrices, often visualized as heatmaps, display the pairwise correlations between numerical variables. They help identify relationships, dependencies, and multicollinearity among variables.\n",
    "\n",
    "\n",
    "These visualizations serve as a starting point, and the choice of specific plots depends on the **nature of the data** and **the questions being explored**. It's important to select visualizations that effectively represent the variables of interest and facilitate the identification of patterns, trends, outliers, and relationships within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71470f-c44c-41ad-bab5-63630d379871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.hist(data_df['column1'], bins=10)\n",
    "plt.xlabel('X-axis label')\n",
    "plt.ylabel('Y-axis label')\n",
    "plt.title('Histogram of Column')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aed0aa-e824-42ce-b7a8-065b90795912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot\n",
    "sns.boxplot(x=data_df['column1'])\n",
    "plt.xlabel('X-axis label')\n",
    "plt.title('Box Plot of Column')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48067dfe-427a-4e6f-a687-efc664059ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.scatter(data_df['column1'], data_df['column2'])\n",
    "plt.xlabel('X-axis label')\n",
    "plt.ylabel('Y-axis label')\n",
    "plt.title('Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc906674-2afb-43d5-b11f-2bde3236bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numeric columns\n",
    "numeric_columns = data_df.select_dtypes(include=[np.number])\n",
    "corr_matrix = numeric_columns.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fb4c7b-bed3-481d-9555-2988076f8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart\n",
    "sns.countplot(data_df['column1'])\n",
    "plt.xlabel('X-axis label')\n",
    "plt.ylabel('Y-axis label')\n",
    "plt.title('Bar Chart')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9c388b1-947c-4e37-8289-93062813370f",
   "metadata": {},
   "source": [
    "## EDA Summary and Conclusion\n",
    "\n",
    "\n",
    "Concluding an Exploratory Data Analysis (EDA) notebook is an important step to summarize the insights gained from the analysis. Here are some key points to consider when concluding an EDA notebook:\n",
    "\n",
    "- **Dataset Summary**: Provide an overview of the dataset, including its size, features, and relevant background information.\n",
    "\n",
    "- **Data Quality and Cleaning**: Highlight any data quality issues encountered and the steps taken to clean and preprocess the data.\n",
    "\n",
    "- **Key Findings**: Summarize the main insights and patterns discovered during the analysis.\n",
    "\n",
    "- **Feature Importance**: Identify the most influential features that have a significant impact on the target variable.\n",
    "\n",
    "- **Outliers and Anomalies**: Discuss any outliers or anomalies identified and their potential impact on the analysis.\n",
    "\n",
    "- **Data Distributions**: Summarize the distribution characteristics of the data for each feature.\n",
    "\n",
    "- **Data Limitations**: Acknowledge any limitations or caveats of the dataset that may affect the analysis or generalizability of the findings.\n",
    "\n",
    "- **Next Steps**: Suggest potential next steps or areas of further investigation based on the EDA.\n",
    "\n",
    "- **Conclusion**: Provide a concise summary of the key insights gained from the EDA.\n",
    "\n",
    "- **Final Thoughts**: Share any additional reflections or recommendations based on the analysis.\n",
    "\n",
    "Remember to keep the language clear and to the point, and provide supporting evidence or visualizations where necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sales_forecasting_env (Local)",
   "language": "python",
   "name": "local-sales_forecasting_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
